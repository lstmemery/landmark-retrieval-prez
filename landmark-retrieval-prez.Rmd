---
title: "Landmark Retrieval Kaggle"
author: "Matt Emery"
date: "08/08/2019"
output: revealjs::revealjs_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Goal


 - You will have a rough understand of how image retrieval works.

<!-- Add more here -->

## Competitions

 - Google Landmark Retrieval 2019
 - Google Landmark Recognition 2019
 
## Data

 - Images in Training: 4,132,914 (0.5 TB)
 - 203,094 Classes (Landmarks)
 - 117,577 test images
 
Not very well curated

There are also distractor images
<!-- Follow up on this. -->

## Intro to CNNs

## Some images are distractors

![](./img/goat.jpg)

## Images are different sizes

![](./img/tiny.jpg)

## Images have different lighting conditions


![](./img/nighttime.jpg)


## Some images are orientated the wrong way

![](./img/wrong-direction.jpg)

## Retrieval Evaluation

 - Mean Average Precision at 100 
 - The model gets 100 chances to retrieval the proper class
 - Reward is $\frac{1}{k}$ where $k$ is the rank of the correct class
 - Take the average of all queries

## Recognition Evaluation

 - Google Landmark Recognition: Global Average Precision

$$GAP = \frac{1}{M}\sum_{i=1}^N P(i) rel(i)$$
 - $M$ is number of query images with landmarks
 - $N$ total number of predictions across all queries
 - $P(i)$ Precision at rank $i$
 - $rel(i)$ an indicator function if the prediction is correct

<!-- How is relevance defined? -->



## Competition Info

 - Reward: $25000 for each competition
 - 

## First Place Solution

 - Yinzheng Gu and Chuanpeng Li of the Video++ AI Lab

### Data Cleaning - Classes

 - Just remove the very imbalanced classes (less than 4 examples)
 - Use the model from last year as a feature "descriptors"
 - A descriptor is a vector represented by a generalized-mean pooling layer (see next slide)
 - Within each class, compare the cosine similarity of each pair
 - Classes with no pairs that agree (cosine similarity > 0.5) are removed

### GeM Pooling Layers

 - First Proposed in 2017

$$\mathbf{f}^{(g)}=\left[\mathrm{f}_{1}^{(g)} \ldots \mathrm{f}_{k}^{(g)} \ldots \mathrm{f}_{K}^{(g)}\right]^{\top}, \quad \mathrm{f}_{k}^{(g)}=\left(\frac{1}{\left|\mathcal{X}_{k}\right|} \sum_{x \in \mathcal{X}_{k}} x^{p_{k}}\right)^{\frac{1}{p_{k}}}$$
 Where $\mathcal{X}_{k}$ is the $k$th kernel of the 3D Image tensor and $p_k$ is the pooling parameter, which can be learned. 
 $p_k = 1$ is global average pooling and $p_k -> \infty$ is global max pooling.
 
---
Notes: https://github.com/filipradenovic/cnnimageretrieval-pytorch

## What does this mean?

![GeM Descriptors](img/gem_descriptors.png)


<aside class="notes">
    Each pair of pictures represents the maximum activations of a different kernel
</aside>


### Data Cleaning - Images

 - Of the images that remain, find the "most agreeable" image (most other images with 0.5 cosine similarity)
 - Only include that image and the images that agree with it
 
 Before Data Cleaning: 4,132,914 Images, 203,094 Classes
 After Data Cleaning: 836,964 Images, 112,782 Classes

## Global CNN Models

 - Used a "Combination of Multiple Global Descriptors" Model

![https://raw.githubusercontent.com/naver/cgd/master/figures/architecture.png](img/architecture.png)
 - Backbone is a ResNet (no max pooling on the 3rd stage)
 - Each global descriptor is a single kernel
 - Connect to a fully connected layer and add $l_2$ regularization
 - Predict ranking loss
 
### Auxilery Classification Loss

 - Only applies to the 1st global descriptor
 - Just a batch norm and temperature scaled softmax <!-- Follow up on this -->

## Attentional Mapping

 - The proposed model concatenates feature maps from several models together
 - Each feature map undergoes GeM plus global average and max pooling and sum-pooled convolutional features <!-- Follow up on sum pooling -->
 - Attention is used to focus the network on the most important features
 - Attention units act as side channels to determine how strong the signal from a single feature is
 <!-- Will need a diagram here -->


## Whitening

 - Attentuated Unsupervised Whitening
 - 
 <!-- More -->
 
 
## Loss Function

 - They actually used two loss functions and summed them together
 - First construct a tuple of a query image, an image from the same class and five images outside of the class
 
 $$\left(I_{q}, I_{p}, I_{n, 1}, \dots, I_{n, 5}\right)$$

### Contrastive loss

 - Clone the network, with the same weights
 - Transform the tuple containing the query image and the positive and negative examples into a list of 2-tuples
 - If the images are in the same class, the loss the half the norm of the difference of two descriptors
 - Otherwise, subtract the difference of the two descriptors from a margin hyperparameter
 
 $$\left(I_{q}, I_{p}\right),\left(I_{q}, I_{n, 1}\right), \ldots,\left(I_{q}, I_{n, 5}\right)$$

$$\mathcal{L}(i, j)=\left\{\begin{array}{ll}{\frac{1}{2}\|\overline{\mathbf{f}}(i)-\overline{\mathbf{f}}(j)\|^{2},} & {\text { if } Y(i, j)=1} \\ {\frac{1}{2}(\max \{0, \tau-\|\overline{\mathbf{f}}(i)-\overline{\mathbf{f}}(j)\|\})^{2},} & {\text { if } Y(i, j)=0}\end{array}\right.$$

### Triplet loss

 - 3-tuples instead of 2-tuples
 - Suggested to be a smoother function

$$\left(I_{q}, I_{p}, I_{n, 1}\right), \ldots,\left(I_{q}, I_{p}, I_{n, 5}\right)$$

$$\mathcal{L}(i)=\sum_{i=1}^{N}\left[\left\|f_{i}^{q}-f_{i}^{p}\right\|^{2}-\left\|f_{i}^{q}-f_{i}^{n}\right\|^{2}+\alpha\right]_{+}$$

 - Triplet loss is very similar to contrastive loss. I don't know why the JL decided to use both losses

## What does a Siamese network look like?

![https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e](./img/siamese-networks.jpeg)

## Siamese Network Code

```python
class SiameseNetwork(nn.Module):
    def __init__(self):
        super(SiameseNetwork, self).__init__()
        self.cnn1 = nn.Sequential(...)

        self.fc1 = nn.Sequential(...)

    def forward_once(self, x):
        output = self.cnn1(x)
        output = output.view(output.size()[0], -1)
        output = self.fc1(output)
        return output

    def forward(self, input1, input2):
        output1 = self.forward_once(input1)
        output2 = self.forward_once(input2)
        return output1, output2

```

## Contrastive Loss

```python
class ContrastiveLoss(torch.nn.Module):
    def __init__(self, margin=2.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)
        loss_contrastive = torch.mean(
            (1-label) * torch.pow(euclidean_distance, 2) + \
            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)
        )

        return loss_contrastive

```


https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/blob/master/Siamese-networks-medium.ipynb


## Local CNN Model

# Bibliography

@article{gu_team_nodate,
	title = {Team {JL} {Solution} to {Google} {Landmark} {Recognition} 2019},
	abstract = {In this paper, we describe our solution to the Google Landmark Recognition 2019 Challenge held on Kaggle. Due to the large number of classes, noisy data, imbalanced class sizes, and the presence of a signiﬁcant amount of distractors in the test set, our method is based mainly on retrieval techniques with both global and local CNN approaches. Our full pipeline, after ensembling the models and applying several steps of re-ranking strategies, scores 0.37606 GAP on the private leaderboard which won the 1st place in the competition.},
	language = {en},
	author = {Gu, Yinzheng and Li, Chuanpeng},
	pages = {4},
	file = {Gu and Li - Team JL Solution to Google Landmark Recognition 20.pdf:/home/deadhead/Zotero/storage/5L8BFMQW/Gu and Li - Team JL Solution to Google Landmark Recognition 20.pdf:application/pdf}
}

@article{radenovic_fine-tuning_2017,
	title = {Fine-tuning {CNN} {Image} {Retrieval} with {No} {Human} {Annotation}},
	url = {http://arxiv.org/abs/1711.02512},
	abstract = {Image descriptors based on activations of Convolutional Neural Networks (CNNs) have become dominant in image retrieval due to their discriminative power, compactness of representation, and search efficiency. Training of CNNs, either from scratch or fine-tuning, requires a large amount of annotated data, where a high quality of annotation is often crucial. In this work, we propose to fine-tune CNNs for image retrieval on a large collection of unordered images in a fully automated manner. Reconstructed 3D models obtained by the state-of-the-art retrieval and structure-from-motion methods guide the selection of the training data. We show that both hard-positive and hard-negative examples, selected by exploiting the geometry and the camera positions available from the 3D models, enhance the performance of particular-object retrieval. CNN descriptor whitening discriminatively learned from the same training data outperforms commonly used PCA whitening. We propose a novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and average pooling and show that it boosts retrieval performance. Applying the proposed method to the VGG network achieves state-of-the-art performance on the standard benchmarks: Oxford Buildings, Paris, and Holidays datasets.},
	urldate = {2019-08-10},
	journal = {arXiv:1711.02512 [cs]},
	author = {Radenović, Filip and Tolias, Giorgos and Chum, Ondřej},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.02512},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: TPAMI 2018. arXiv admin note: substantial text overlap with arXiv:1604.02426},
	file = {arXiv\:1711.02512 PDF:/home/deadhead/Zotero/storage/87BYWUY6/Radenović et al. - 2017 - Fine-tuning CNN Image Retrieval with No Human Anno.pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/XCI389IH/1711.html:text/html}
}

@article{jun_combination_2019,
	title = {Combination of {Multiple} {Global} {Descriptors} for {Image} {Retrieval}},
	url = {http://arxiv.org/abs/1903.10663},
	abstract = {Recent studies in image retrieval task have shown that ensembling different models and combining multiple global descriptors lead to performance improvement. However, training different models for the ensemble is not only difficult but also inefficient with respect to time and memory. In this paper, we propose a novel framework that exploits multiple global descriptors to get an ensemble effect while it can be trained in an end-to-end manner. The proposed framework is flexible and expandable by the global descriptor, CNN backbone, loss, and dataset. Moreover, we investigate the effectiveness of combining multiple global descriptors with quantitative and qualitative analysis. Our extensive experiments show that the combined descriptor outperforms a single global descriptor, as it can utilize different types of feature properties. In the benchmark evaluation, the proposed framework achieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop Clothes, and Stanford Online Products on image retrieval tasks. Our model implementations and pretrained models are publicly available.},
	urldate = {2019-08-11},
	journal = {arXiv:1903.10663 [cs]},
	author = {Jun, HeeJae and Ko, ByungSoo and Kim, Youngjoon and Kim, Insik and Kim, Jongtack},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.10663},
	keywords = {Computer Science - Information Retrieval, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv\:1903.10663 PDF:/home/deadhead/Zotero/storage/4PVIMZCI/Jun et al. - 2019 - Combination of Multiple Global Descriptors for Ima.pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/6H8T6WQI/1903.html:text/html}
}

@article{gu_attention-aware_2018,
	title = {Attention-{Aware} {Generalized} {Mean} {Pooling} for {Image} {Retrieval}},
	url = {http://arxiv.org/abs/1811.00202},
	abstract = {It has been shown that image descriptors extracted by convolutional neural networks (CNNs) achieve remarkable results for retrieval problems. In this paper, we apply attention mechanism to CNN, which aims at enhancing more relevant features that correspond to important keypoints in the input image. The generated attention-aware features are then aggregated by the previous state-of-the-art generalized mean (GeM) pooling followed by normalization to produce a compact global descriptor, which can be efficiently compared to other image descriptors by the dot product. An extensive comparison of our proposed approach with state-of-the-art methods is performed on the new challenging ROxford5k and RParis6k retrieval benchmarks. Results indicate significant improvement over previous work. In particular, our attention-aware GeM (AGeM) descriptor outperforms state-of-the-art method on ROxford5k under the `Hard' evaluation protocal.},
	urldate = {2019-08-11},
	journal = {arXiv:1811.00202 [cs]},
	author = {Gu, Yinzheng and Li, Chuanpeng and Xie, Jinbin},
	month = oct,
	year = {2018},
	note = {arXiv: 1811.00202},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Shortened version for submission},
	file = {arXiv\:1811.00202 PDF:/home/deadhead/Zotero/storage/IIYXGGDF/Gu et al. - 2018 - Attention-Aware Generalized Mean Pooling for Image.pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/6EQB5JSE/1811.html:text/html}
}

@article{tolias_particular_2015,
	title = {Particular object retrieval with integral max-pooling of {CNN} activations},
	url = {http://arxiv.org/abs/1511.05879},
	abstract = {Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image re-ranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets.},
	urldate = {2019-08-11},
	journal = {arXiv:1511.05879 [cs]},
	author = {Tolias, Giorgos and Sicre, Ronan and Jégou, Hervé},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05879},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1511.05879 PDF:/home/deadhead/Zotero/storage/8C35IS5P/Tolias et al. - 2015 - Particular object retrieval with integral max-pool.pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/I52ZCVQT/1511.html:text/html}
}

@article{olah_attention_2016,
	title = {Attention and {Augmented} {Recurrent} {Neural} {Networks}},
	volume = {1},
	issn = {2476-0757},
	url = {http://distill.pub/2016/augmented-rnns},
	doi = {10.23915/distill.00001},
	abstract = {A visual overview of neural attention, and the powerful extensions of neural networks being built on top of it.},
	language = {en},
	number = {9},
	urldate = {2019-08-11},
	journal = {Distill},
	author = {Olah, Chris and Carter, Shan},
	month = sep,
	year = {2016},
	pages = {e1},
	file = {Snapshot:/home/deadhead/Zotero/storage/47K6BFEE/augmented-rnns.html:text/html}
}

@article{babenko_aggregating_2015,
	title = {Aggregating {Deep} {Convolutional} {Features} for {Image} {Retrieval}},
	url = {http://arxiv.org/abs/1510.07493},
	abstract = {Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It has also been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregation approaches developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptors. In this paper we investigate possible ways to aggregate local deep features to produce compact global descriptors for image retrieval. First, we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities, hence existing aggregation methods have to be carefully re-evaluated. Such re-evaluation reveals that in contrast to shallow features, the simple aggregation method based on sum pooling provides arguably the best performance for deep convolutional features. This method is efficient, has few parameters, and bears little risk of overfitting when e.g. learning the PCA matrix. Overall, the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably.},
	urldate = {2019-08-11},
	journal = {arXiv:1510.07493 [cs]},
	author = {Babenko, Artem and Lempitsky, Victor},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.07493},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted for ICCV 2015},
	file = {arXiv\:1510.07493 PDF:/home/deadhead/Zotero/storage/PX9Q8I98/Babenko and Lempitsky - 2015 - Aggregating Deep Convolutional Features for Image .pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/7NDLRCFW/1510.html:text/html}
}

@article{gu_team_2019,
	title = {Team {JL} {Solution} to {Google} {Landmark} {Recognition} 2019},
	url = {http://arxiv.org/abs/1906.11874},
	abstract = {In this paper, we describe our solution to the Google Landmark Recognition 2019 Challenge held on Kaggle. Due to the large number of classes, noisy data, imbalanced class sizes, and the presence of a significant amount of distractors in the test set, our method is based mainly on retrieval techniques with both global and local CNN approaches. Our full pipeline, after ensembling the models and applying several steps of re-ranking strategies, scores 0.37606 GAP on the private leaderboard which won the 1st place in the competition.},
	urldate = {2019-08-13},
	journal = {arXiv:1906.11874 [cs]},
	author = {Gu, Yinzheng and Li, Chuanpeng},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.11874},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1906.11874 PDF:/home/deadhead/Zotero/storage/HPL2XQVY/Gu and Li - 2019 - Team JL Solution to Google Landmark Recognition 20.pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/QMQXBNCC/1906.html:text/html}
}

@article{arandjelovic_netvlad:_2015,
	title = {{NetVLAD}: {CNN} architecture for weakly supervised place recognition},
	shorttitle = {{NetVLAD}},
	url = {http://arxiv.org/abs/1511.07247},
	abstract = {We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the "Vector of Locally Aggregated Descriptors" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.},
	urldate = {2019-08-15},
	journal = {arXiv:1511.07247 [cs]},
	author = {Arandjelović, Relja and Gronat, Petr and Torii, Akihiko and Pajdla, Tomas and Sivic, Josef},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.07247},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Appears in: IEEE Computer Vision and Pattern Recognition (CVPR) 2016},
	file = {arXiv\:1511.07247 PDF:/home/deadhead/Zotero/storage/SHH4I4QR/Arandjelović et al. - 2015 - NetVLAD CNN architecture for weakly supervised pl.pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/9G3Q3PJS/1511.html:text/html}
}

@article{ozaki_large-scale_2019,
	title = {Large-scale {Landmark} {Retrieval}/{Recognition} under a {Noisy} and {Diverse} {Dataset}},
	url = {http://arxiv.org/abs/1906.04087},
	abstract = {The Google-Landmarks-v2 dataset is the biggest worldwide landmarks dataset characterized by a large magnitude of noisiness and diversity. We present a novel landmark retrieval/recognition system, robust to a noisy and diverse dataset, by our team, smlyaka. Our approach is based on deep convolutional neural networks with metric learning, trained by cosine-softmax based losses. Deep metric learning methods are usually sensitive to noise, and it could hinder to learn a reliable metric. To address this issue, we develop an automated data cleaning system. Besides, we devise a discriminative re-ranking method to address the diversity of the dataset for landmark retrieval. Using our methods, we achieved 1st place in the Google Landmark Retrieval 2019 challenge and 3rd place in the Google Landmark Recognition 2019 challenge on Kaggle.},
	urldate = {2019-08-17},
	journal = {arXiv:1906.04087 [cs]},
	author = {Ozaki, Kohei and Yokoo, Shuhei},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04087},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Technical report for Second Landmark Recognition Workshop to be held at CVPR 2019 (Long Beach, CA), June 16th, 2019},
	file = {arXiv\:1906.04087 PDF:/home/deadhead/Zotero/storage/TS37H2YI/Ozaki and Yokoo - 2019 - Large-scale Landmark RetrievalRecognition under a.pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/YU3AZGRM/1906.html:text/html}
}

@inproceedings{perronnin_family_2009,
	title = {A family of contextual measures of similarity between distributions with application to image retrieval},
	doi = {10.1109/CVPR.2009.5206505},
	abstract = {We introduce a novel family of contextual measures of similarity between distributions: the similarity between two distributions q and p is measured in the context of a third distribution u. In our framework any traditional measure of similarity / dissimilarity has its contextual counterpart. We show that for two important families of divergences (Bregman and Csisz'ar), the contextual similarity computation consists in solving a convex optimization problem. We focus on the case of multinomials and explain how to compute in practice the similarity for several well-known measures. These contextual measures are then applied to the image retrieval problem. In such a case, the context u is estimated from the neighbors of a query q. One of the main benefits of our approach lies in the fact that using different contexts, and especially contexts at multiple scales (i.e. broad and narrow contexts), provides different views on the same problem. Combining the different views can improve retrieval accuracy. We will show on two very different datasets (one of photographs, the other of document images) that the proposed measures have a relatively small positive impact on macro Average Precision (which measures purely ranking) and a large positive impact on micro Average Precision (which measures both ranking and consistency of the scores across multiple queries).},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Perronnin, F. and Liu, Y. and Renders, J.},
	month = jun,
	year = {2009},
	keywords = {Animals, Humans, Europe, Cats, contextual measure, contextual similarity computation, convex optimization problem, convex programming, image retrieval, Image retrieval, image retrieval problem, Information retrieval, macro average precision, micro average precision, Painting, Pattern analysis, Q measurement, Rendering (computer graphics)},
	pages = {2358--2365},
	file = {IEEE Xplore Abstract Record:/home/deadhead/Zotero/storage/FILU5CW5/5206505.html:text/html}
}

@misc{noauthor_one_nodate,
	title = {One {Shot} {Learning} with {Siamese} {Networks} in {PyTorch}},
	url = {https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e},
	language = {en},
	urldate = {2019-08-17},
	file = {Snapshot:/home/deadhead/Zotero/storage/EMLVRQYD/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e.html:text/html}
}

@misc{gupta_implementing_2019,
	title = {Implementing {Siamese} networks with a contrastive loss for similarity learning: harveyslash/{Facial}-{Similarity}-with-{Siamese}-{Networks}-in-{Pytorch}},
	shorttitle = {Implementing {Siamese} networks with a contrastive loss for similarity learning},
	url = {https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch},
	urldate = {2019-08-17},
	author = {Gupta, Harshvardhan},
	month = aug,
	year = {2019},
	note = {original-date: 2017-07-19T04:10:30Z}
}

@article{teichmann_detect--retrieve:_2018,
	title = {Detect-to-{Retrieve}: {Efficient} {Regional} {Aggregation} for {Image} {Search}},
	shorttitle = {Detect-to-{Retrieve}},
	url = {http://arxiv.org/abs/1812.01584},
	abstract = {Retrieving object instances among cluttered scenes efficiently requires compact yet comprehensive regional image representations. Intuitively, object semantics can help build the index that focuses on the most relevant regions. However, due to the lack of bounding-box datasets for objects of interest among retrieval benchmarks, most recent work on regional representations has focused on either uniform or class-agnostic region selection. In this paper, we first fill the void by providing a new dataset of landmark bounding boxes, based on the Google Landmarks dataset, that includes \$86k\$ images with manually curated boxes from \$15k\$ unique landmarks. Then, we demonstrate how a trained landmark detector, using our new dataset, can be leveraged to index image regions and improve retrieval accuracy while being much more efficient than existing regional methods. In addition, we introduce a novel regional aggregated selective match kernel (R-ASMK) to effectively combine information from detected regions into an improved holistic image representation. R-ASMK boosts image retrieval accuracy substantially with no dimensionality increase, while even outperforming systems that index image regions independently. Our complete image retrieval system improves upon the previous state-of-the-art by significant margins on the Revisited Oxford and Paris datasets. Code and data available at the project webpage: https://github.com/tensorflow/models/tree/master/research/delf.},
	urldate = {2019-08-18},
	journal = {arXiv:1812.01584 [cs]},
	author = {Teichmann, Marvin and Araujo, Andre and Zhu, Menglong and Sim, Jack},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01584},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2019. Code and dataset available: https://github.com/tensorflow/models/tree/master/research/delf},
	file = {arXiv\:1812.01584 PDF:/home/deadhead/Zotero/storage/N9NASJ59/Teichmann et al. - 2018 - Detect-to-Retrieve Efficient Regional Aggregation.pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/LAF53W4G/1812.html:text/html}
}

@article{noh_large-scale_2016,
	title = {Large-{Scale} {Image} {Retrieval} with {Attentive} {Deep} {Local} {Features}},
	url = {http://arxiv.org/abs/1612.06321},
	abstract = {We propose an attentive local feature descriptor suitable for large-scale image retrieval, referred to as DELF (DEep Local Feature). The new feature is based on convolutional neural networks, which are trained only with image-level annotations on a landmark image dataset. To identify semantically useful local features for image retrieval, we also propose an attention mechanism for keypoint selection, which shares most network layers with the descriptor. This framework can be used for image retrieval as a drop-in replacement for other keypoint detectors and descriptors, enabling more accurate feature matching and geometric verification. Our system produces reliable confidence scores to reject false positives---in particular, it is robust against queries that have no correct match in the database. To evaluate the proposed descriptor, we introduce a new large-scale dataset, referred to as Google-Landmarks dataset, which involves challenges in both database and query such as background clutter, partial occlusion, multiple landmarks, objects in variable scales, etc. We show that DELF outperforms the state-of-the-art global and local descriptors in the large-scale setting by significant margins. Code and dataset can be found at the project webpage: https://github.com/tensorflow/models/tree/master/research/delf .},
	urldate = {2019-08-18},
	journal = {arXiv:1612.06321 [cs]},
	author = {Noh, Hyeonwoo and Araujo, Andre and Sim, Jack and Weyand, Tobias and Han, Bohyung},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.06321},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV 2017. Code and dataset available: https://github.com/tensorflow/models/tree/master/research/delf},
	file = {arXiv\:1612.06321 PDF:/home/deadhead/Zotero/storage/C2E5ED2J/Noh et al. - 2016 - Large-Scale Image Retrieval with Attentive Deep Lo.pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/AJ8BVYCH/1612.html:text/html}
}

@misc{noauthor_computer_nodate,
	title = {Computer {Vision}: {What} is the difference between local descriptors and global descriptors? - {Quora}},
	url = {https://www.quora.com/Computer-Vision-What-is-the-difference-between-local-descriptors-and-global-descriptors},
	urldate = {2019-08-18},
	file = {Computer Vision\: What is the difference between local descriptors and global descriptors? - Quora:/home/deadhead/Zotero/storage/8Q6SYPT9/Computer-Vision-What-is-the-difference-between-local-descriptors-and-global-descriptors.html:text/html}
}

@article{tolias_image_2016,
	title = {Image {Search} with {Selective} {Match} {Kernels}: {Aggregation} {Across} {Single} and {Multiple} {Images}},
	volume = {116},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Image {Search} with {Selective} {Match} {Kernels}},
	url = {https://link.springer.com/article/10.1007/s11263-015-0810-4},
	doi = {10.1007/s11263-015-0810-4},
	abstract = {This paper considers a family of metrics to compare images based on their local descriptors. It encompasses the vector or locally aggregated descriptors descriptor and matching techniques such as...},
	language = {en},
	number = {3},
	urldate = {2019-08-18},
	journal = {International Journal of Computer Vision},
	author = {Tolias, Giorgos and Avrithis, Yannis and Jégou, Hervé},
	month = feb,
	year = {2016},
	pages = {247--261},
	file = {Snapshot:/home/deadhead/Zotero/storage/LZFIAKIU/s11263-015-0810-4.html:text/html}
}

@misc{noauthor_recognition_slides.pdf_nodate,
	title = {recognition\_slides.pdf},
	url = {https://drive.google.com/file/d/14PdHtRBXE3DTYxu5fpoDtTZUTm0Z9MXr/view?usp=drive_open&usp=embed_facebook},
	urldate = {2019-08-18},
	journal = {Google Docs},
	file = {Snapshot:/home/deadhead/Zotero/storage/38849HCU/view.html:text/html}
}

@misc{jose_ramon_rios_viqueira_information_nodate,
	type = {Datos y análisis},
	title = {Information {Retrieval} {Evaluation}},
	url = {https://es.slideshare.net/JosRamnRosViqueira/information-retrieval-evaluation-64158263},
	abstract = {Tutorial given at the 2nd KEYSTONE Training School. Keyword search in Big},
	urldate = {2019-08-22},
	author = {José Ramón Ríos Viqueira}
}

@misc{noauthor_attention?_2018,
	title = {Attention? {Attention}!},
	shorttitle = {Attention?},
	url = {https://lilianweng.github.io/2018/06/24/attention-attention.html},
	abstract = {Attention has been a fairly popular concept and a useful tool in the deep learning community in recent years. In this post, we are gonna look into how attention was invented, and various attention mechanisms and models, such as transformer and SNAIL.},
	language = {en},
	urldate = {2019-08-22},
	journal = {Lil'Log},
	month = jun,
	year = {2018}
}

@misc{prove_squeeze-and-excitation_2017,
	title = {Squeeze-and-{Excitation} {Networks}},
	url = {https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7},
	abstract = {Setting a new state of the art on ImageNet},
	language = {en},
	urldate = {2019-08-22},
	journal = {Medium},
	author = {Pröve, Paul-Louis},
	month = oct,
	year = {2017},
	file = {Snapshot:/home/deadhead/Zotero/storage/NHAJM7LA/squeeze-and-excitation-networks-9ef5e71eacd7.html:text/html}
}

@article{hu_squeeze-and-excitation_2017,
	title = {Squeeze-and-{Excitation} {Networks}},
	url = {http://arxiv.org/abs/1709.01507},
	abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of {\textasciitilde}25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
	urldate = {2019-08-22},
	journal = {arXiv:1709.01507 [cs]},
	author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.01507},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: journal version of the CVPR 2018 paper, accepted by TPAMI},
	file = {arXiv\:1709.01507 PDF:/home/deadhead/Zotero/storage/RLKCFCTR/Hu et al. - 2017 - Squeeze-and-Excitation Networks.pdf:application/pdf;arXiv.org Snapshot:/home/deadhead/Zotero/storage/YQHCWKWX/1709.html:text/html}
}

@misc{arsenault_lossless_2018,
	title = {Lossless {Triplet} loss},
	url = {https://towardsdatascience.com/lossless-triplet-loss-7e932f990b24},
	abstract = {A more efficient loss function for Siamese NN},
	language = {en},
	urldate = {2019-08-22},
	journal = {Medium},
	author = {Arsenault, Marc-Olivier},
	month = feb,
	year = {2018},
	file = {Snapshot:/home/deadhead/Zotero/storage/FQSRKBS6/lossless-triplet-loss-7e932f990b24.html:text/html}
}
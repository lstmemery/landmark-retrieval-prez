---
title: "Landmark Retrieval Kaggle"
author: "Matt Emery"
date: "08/08/2019"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Competitions

 - Google Landmark Retrieval 2019
 - Google Landmark Recognition 2019
 
## Data

 - Images in Training 4,132,914
 - 203,094 Classes (Landmarks)
 - 117,577 test images
 
Not very well curated

There are also distractor images
<!-- Follow up on this. -->

## Evaluation

 - Google Landmark Recognition: Global Average Precision

$$GAP = \frac{1}{M}\sum_{i=1}^N P(i) rel(i)$$

 - Google Landmark Retrieval: Mean Average Precision at 100 

$$mAP@100 = \frac{1}{Q} \sum{q=1}^{Q} \frac{1}{min(mq, 100)} \sum{k=1}^{min(nq,100)} Pq(k) relq(k)$$

## Award

 - 25K each

## First Place Solution

### Data Cleaning

 - Just remove the very imbalanced classes (less than 4 examples)
 - Use the model from last year as a feature descriptors <!-- What do they mean by descriptors? -->
 <!-- Fine-tuning CNN Image Retrieval with No Human Annotation -->
 - If from within a class there was no agreement, then remove that class as well <!-- Follow up on this -->